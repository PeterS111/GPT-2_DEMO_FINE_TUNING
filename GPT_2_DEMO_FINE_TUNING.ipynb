{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WORX GPT-2_DEMO_FINE_TUNING.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterS111/GPT-2_DEMO_FINE_TUNING/blob/main/WORX_GPT_2_DEMO_FINE_TUNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAfruuXn1RJE"
      },
      "source": [
        "\r\n",
        "# GPT-2 Base Fine-Tuning Demo\r\n",
        "version 10.12.2020\r\n",
        "\r\n",
        "The code is based on https://github.com/priya-dwivedi/Deep-Learning/ by N Shepperd, with small changes by Peter S\r\n",
        "\r\n",
        "This notebook allows to fine-tune GPT-2 Small and GPT-2 Medium and generate text from them. \r\n",
        "\r\n",
        "First make sure that you are using a GPU: Runtime/Change runtime type/ -> Select \"GPU\". There are four types of GPUs on Colab: K80, P4, T4 and P100. If you want to fine-tune GPT-2 Medium you will need T4 or P100. For the GPT-2 Small any GPU will work. To check which GPU you have, run the command below. If the GPU is not the one you require go to \"Runtime/Factory reset runtime\" and run the command below again. Usually after a few \"resets\" you will get either T4 or P100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfnKQwQQ1_gk"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98IxMgje2HZT"
      },
      "source": [
        "# 2. Download the repository from github:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3Qv6nfmrXPr"
      },
      "source": [
        "!git clone https://github.com/PeterS111/GPT-2_DEMO_FINE_TUNING  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLC20YbW2NcD"
      },
      "source": [
        "# 3. Change the working directory to the main folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_xfH-FJrXne"
      },
      "source": [
        "cd /content/GPT-2_DEMO_FINE_TUNING"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWcBZeOR2jsb"
      },
      "source": [
        "# 4. Install Tranformers library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07S832Y0rX1y"
      },
      "source": [
        "pip install \"transformers==2.7.0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLd4egEA2iUz"
      },
      "source": [
        "# 5. Training the model without validation dataset:\r\n",
        "\r\n",
        "You can train the model with or without the validation dataset.\r\n",
        "Datasets of Tolstoy's novels and Shelley's are provided with the notebook. You can of course replace those datasets with your own. In that case you will have to upload them to /content/GPT-2_DEMO_FINE_TUNING/input_data. You can \"drag and drop\" them from your PC. You will have to change the argument: \"--train_data_file\" accordingly.\r\n",
        "\r\n",
        "To control the training time you can change the number of epochs.\r\n",
        "\r\n",
        "To train the Medium model change the following argument:\r\n",
        "\r\n",
        "--model_name_or_path=gpt2\r\n",
        "\r\n",
        "to:\r\n",
        "\r\n",
        "--model_name_or_path=gpt2-medium\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryAY2670rYEJ"
      },
      "source": [
        "!python run_lm_finetuning.py --output_dir=output  --model_type=gpt2 --model_name_or_path=gpt2 --do_train  --train_data_file=input_data/Shelley.txt --overwrite_output_dir --block_size=200 --per_gpu_train_batch_size=1 --save_steps 1000 --save_total_limit 1 --num_train_epochs=5 --logging_steps=5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIwoiOLH6hEy"
      },
      "source": [
        "# 6. Training the model with validation dataset:\r\n",
        "\r\n",
        "The same as above but if you want to used your own datasets you will have to change the \"--eval_data_file\" and \"--train_data_file\" arguments accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riej0YolrYVt"
      },
      "source": [
        "!python run_lm_finetuning.py --output_dir=output  --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=input_data/Shelley_train.txt --do_eval --eval_data_file=input_data/Shelley_val.txt --overwrite_output_dir --block_size=200 --per_gpu_train_batch_size=1 --save_steps 1000 --save_total_limit 1 --num_train_epochs=5 --logging_steps=5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpHMqWDV7sQR"
      },
      "source": [
        "# 7. Generate text:\r\n",
        "\r\n",
        "First check the folder /GPT-2_DEMO_FINE_TUNING/output for a folder named \"checkpoint- *\". In our case the folder is \"checkpoint-6000\". If you used a different dataset or change the training time, the folder may be different. Make sure that the parameter \"--model_name_or_path output\" has the correct argument. You can change the \"--prompt\" parameter to any continuous string. If you want to generate more samples you can change the \"--num_samples\" parameter.\r\n",
        "\r\n",
        "Generated text will appear in the output cell and a copy will be saved in the \"GPT-2_DEMO_FINE_TUNING/outputs\" folder.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAbhLuferZKe"
      },
      "source": [
        "!python run_generation.py --model_type gpt2 --temperature 1.0 --top_k 50 --top_p 1.0 --model_name_or_path output/checkpoint-6000 --length 500 --prompt \"The blue sky\" --seed 37 --num_samples 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwCHsizsNDEU"
      },
      "source": [
        "# 8. Exporting the fine-tuned model to Google Drive\r\n",
        "\r\n",
        "If you want to save the fine-tuned model for later use, follow these steps:\r\n",
        "\r\n",
        "8.1. Zip the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psapBGyyvCVs"
      },
      "source": [
        "!tar -czvf \"my_model_6000_steps.tar.gz\" output/checkpoint-6000/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjhvwImaNoQb"
      },
      "source": [
        "8.2. Mount Google Drive (it will require authentication)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjG5YkiNx1jx"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKCvk7FuNzKC"
      },
      "source": [
        "8.3. Export the compressed model to Drive. This only takes few minutes, but sometimes you may have to run the command twice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBjg7yXrPJri"
      },
      "source": [
        "!cp \"my_model_6000_steps.tar.gz\" \"/content/drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuM51wb7OG47"
      },
      "source": [
        "#9. Importing saved model from Google Drive\r\n",
        "\r\n",
        "If you want to return to saved model, do the following:\r\n",
        "\r\n",
        "First, run points 2, 3 and 4 (download the repository, change the directory, install Transformers), the same as above. Then:\r\n",
        "\r\n",
        "9.1. Create the output folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9xw7VNmvDGH"
      },
      "source": [
        "!mkdir output/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPCGLRuoPdkC"
      },
      "source": [
        " 9.2. Mount Google Drive, the same as 8.2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v0d02YBPk3q"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOcpDErrPoAD"
      },
      "source": [
        "9.3. Import the previously saved model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrF-v_SZ232k"
      },
      "source": [
        "!cp \"/content/drive/My Drive/my_model_6000_steps.tar.gz\" /content/GPT-2_DEMO_FINE_TUNING/output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF_kVCptPuGy"
      },
      "source": [
        "9.4. Unpack the model and remove the .tar file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xyKdKbzvDUT"
      },
      "source": [
        "!tar xf /content/GPT-2_DEMO_FINE_TUNING/output/my_model_6000_steps.tar.gz\n",
        "!rm -v /content/GPT-2_DEMO_FINE_TUNING/output/my_model_6000_steps.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4xxxQKjQL0c"
      },
      "source": [
        "9.5. Ready to generate text. The command below is the same as in point 7, check there for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBgJnJB5vDj_"
      },
      "source": [
        "!python run_generation.py --model_type gpt2 --temperature 1.0 --top_k 50 --top_p 1.0 --model_name_or_path output/checkpoint-6000 --length 500 --prompt \"The blue sky\" --seed 37 --num_samples 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
